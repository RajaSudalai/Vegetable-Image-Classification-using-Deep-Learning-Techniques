{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "import urllib.request\n",
    "from urllib.request import urlretrieve\n",
    "#import urlib.request as request\n",
    "import requests\n",
    "\n",
    "folder = r'C:\\Users\\jayas\\Test Folder\\Icons'+'\\\\'\n",
    "URL = 'https://www.pexels.com/search/healthy%20food/'\n",
    "page = requests.get(URL) \n",
    "soup = BeautifulSoup(page.content) \n",
    "#response = request.urlopen(URL)\n",
    "#soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "iconTable = soup.find('div', {'class':'hide-featured-badge hide-favorite-badge'})\n",
    "icons = iconTable.find_all('article')\n",
    "\n",
    "for icon in icons:\n",
    "    urlretrieve(icon.img['data-big-src'],folder+icon.img['alt']+'.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "class AppURLopener(urllib.request.FancyURLopener):\n",
    "    version = \"Chrome/76.0\"\n",
    "\n",
    "opener = AppURLopener()\n",
    "response = opener.open('http://httpbin.org/user-agent')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "folder = r'C:\\Users\\jayas\\Test Folder\\Tulips'+'\\\\'\n",
    "#URL = 'https://pixabay.com/images/search/tulips/'\n",
    "URL = 'https://www.shutterstock.com/search/unhealthy+food'\n",
    "page = requests.get(URL) \n",
    "soup = BeautifulSoup(page.content)\n",
    "#print(soup.getText())\n",
    "#response = request.urlopen(URL)\n",
    "#soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "divdata = soup.find('div', {'class': 's_k_a'})\n",
    "divdat = divdata.find('img', {'class': 'z_g_h'})\n",
    "for dat in divdat:\n",
    "    #soup.find_all('a'):\n",
    "    if dat.img:\n",
    "        print(dat.img['src'])\n",
    "\n",
    "\n",
    "'''\n",
    "icons = iconTable.find_all('li')\n",
    "for dat in divdat:\n",
    "    #soup.find_all('a'):\n",
    "    if dat.img:\n",
    "        #print(dat.img['src'])\n",
    "        urlretrieve(dat.img['src'],folder+dat.img['alt']+'.jpeg')\n",
    "'''\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#pixaboy images\n",
    "divdata = soup.find('div', {\"class\": \"media_list\"})\n",
    "#divdat = divdata.find('div', {'class': 'flex_grid sponsored_images'})\n",
    "divdat = divdata.findAll('div', {'class': 'item'})\n",
    "\n",
    "\n",
    "for dat in divdat:\n",
    "    #soup.find_all('a'):\n",
    "    if dat.img:\n",
    "        #print(dat.img['src'])\n",
    "        urlretrieve(dat.img['src'],folder+dat.img['alt']+'.jpeg')\n",
    "'''        \n",
    "\n",
    "'''\n",
    "for div in divdat:\n",
    "    print(div.img.get('src'))\n",
    "'''   \n",
    "'''\n",
    "for icon in divdat:\n",
    "   urlretrieve(icon.img,folder+icon.img['alt']+'.jpeg')\n",
    "#print(div.img.get('src'))\n",
    "'''\n",
    "\n",
    "'''    \n",
    "for getimgtag in divdat.find('img',src=True):\n",
    "    print(getimgtag['src'])\n",
    "\n",
    "all_links = soup.find_all(\"div\")\n",
    "for link in all_links:\n",
    "   print(link.get(\"a\"))\n",
    "\n",
    "iconTable = soup.find('div', {'class':'media_list'})\n",
    "icons = iconTable.find_all('div')\n",
    "\n",
    "for icon in icons:\n",
    "    urlretrieve(icon.img['src'],folder+icon.img['alt']+'.jpeg')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Usage: python scrape_marathon_images --fname <filepath of the img_pg_links CSV>\n",
    "'''\n",
    "\n",
    "# import the necessary packages\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import urllib.request\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "#Define the argument parser to read in the URL\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-fname', '--fname', help='CSV of image links')\n",
    "args = vars(parser.parse_args())\n",
    "f_name = args['fname']\n",
    "\n",
    "# Create the directory name where the images will be saved\n",
    "base_dir = os.getcwd()\n",
    "dir_name = (f_name.split('/')[-1]).split('.')[0]\n",
    "dir_path = os.path.join(base_dir, dir_name)\n",
    "\n",
    "#Create the directory if already not there\n",
    "if not os.path.exists(dir_path):\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "# Read the csv with links to all the image pages\n",
    "f = open(os.path.join(base_dir, f_name),'r')\n",
    "links = f.read().split(',')\n",
    "\n",
    "# Print the number of images\n",
    "print (\"[INFO] Downloading {} images\".format(len(links)))\n",
    "\n",
    "# Function to take an image url and save the image in the given directory\n",
    "def download_image(url):\n",
    "    print(\"[INFO] downloading {}\".format(url))\n",
    "    name = str(url.split('/')[-1])\n",
    "    urllib.request.urlretrieve(url,os.path.join(dir_path, name))\n",
    "\n",
    "for href in links:\n",
    "    # Extract the contents of the link\n",
    "    img_page = requests.get(href)\n",
    "    soup = bs(img_page.content, 'html.parser')\n",
    "    img_class = soup.find_all('meta', attrs={\"name\":\"twitter:image\"})\n",
    "    img_url = img_class[0].get('content')\n",
    "    real_url = img_url.replace('XL', 'X3', 2)\n",
    "    download_image(real_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypexels import PyPexels\n",
    "api_key = '563492ad6f917000010000012eafb7434a734131a6c80cf1c60b2898'\n",
    "from urllib.request import urlretrieve\n",
    "# instantiate PyPexels object\n",
    "py_pexels = PyPexels(api_key=api_key)\n",
    "#folder = r'C:\\Users\\jayas\\Test Folder\\Tulips'+'\\\\'\n",
    "\n",
    "#popular_photos = py_pexels.popular(per_page=30)\n",
    "popular_photos=py_pexels.search(query='healthy food', per_page=40)\n",
    "while popular_photos.has_next:\n",
    "    for photo in popular_photos.entries:\n",
    "        print(photo.id, photo.photographer, photo.url)\n",
    "        print(photo.src.get('large'))\n",
    "        print(photo.src.get('tiny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "#from urllib.request import urlopen\n",
    "import urllib.request\n",
    "from urllib.request import urlretrieve\n",
    "#import urlib.request as request\n",
    "import requests\n",
    "\n",
    "folder = r'C:\\Users\\jayas\\Test Folder\\Icons'+'\\\\'\n",
    "URL = 'https://www.google.com/about/products/'\n",
    "page = requests.get(URL) \n",
    "soup = BeautifulSoup(page.content) \n",
    "#response = request.urlopen(URL)\n",
    "#soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "iconTable = soup.find('ul', {'class':'product-icon-list'})\n",
    "icons = iconTable.find_all('li')\n",
    "\n",
    "for icon in icons:\n",
    "    urlretrieve(icon.img['data-lazy-src'],folder+icon.img['alt']+'.jpeg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
